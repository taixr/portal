<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-generative-ai/stable-diffusion" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">第四章 Stable Diffusion | Taixr</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" property="og:image" content="https://i.taixr.com/logo.jpeg"><meta data-rh="true" name="twitter:image" content="https://i.taixr.com/logo.jpeg"><meta data-rh="true" property="og:url" content="https://www.taixr.com/en/docs/generative-ai/stable-diffusion"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="ai, 人工智能, AR, VR, XR, MR, 增强现实，虚拟现实, 混合现实, visionOS, Vision Pro"><meta data-rh="true" name="twitter:card" content="https://i.taixr.com/logo.jpeg"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="第四章 Stable Diffusion | Taixr"><meta data-rh="true" name="description" content="在前一章中，我们介绍了扩散模型及其迭代优化的基本思想。学完该章，我们已经能够生成图像，但训练模型非常耗时，而且我们无法控制生成的图像。在本章中，我们将学习如何从这一阶段走向基于文本条件的模型，这些模型可以根据文本描述高效地生成图像，研究的是一个名为Stable Diffusion（SD）的模型。不过在介绍SD之前，我们会先了解条件模型如何工作，并回顾一些产生当今文生图像模型的创新。"><meta data-rh="true" property="og:description" content="在前一章中，我们介绍了扩散模型及其迭代优化的基本思想。学完该章，我们已经能够生成图像，但训练模型非常耗时，而且我们无法控制生成的图像。在本章中，我们将学习如何从这一阶段走向基于文本条件的模型，这些模型可以根据文本描述高效地生成图像，研究的是一个名为Stable Diffusion（SD）的模型。不过在介绍SD之前，我们会先了解条件模型如何工作，并回顾一些产生当今文生图像模型的创新。"><link data-rh="true" rel="icon" href="https://i.taixr.com/logo.jpeg"><link data-rh="true" rel="canonical" href="https://www.taixr.com/en/docs/generative-ai/stable-diffusion"><link data-rh="true" rel="alternate" href="https://www.taixr.com/docs/generative-ai/stable-diffusion" hreflang="zh"><link data-rh="true" rel="alternate" href="https://www.taixr.com/en/docs/generative-ai/stable-diffusion" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.taixr.com/docs/generative-ai/stable-diffusion" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://PEDULFQVAW-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="Taixr RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="Taixr Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K4CD0WFGFG"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-K4CD0WFGFG",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Taixr" href="/en/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.4f939f39.css">
<script src="/en/assets/js/runtime~main.4739f96c.js" defer="defer"></script>
<script src="/en/assets/js/main.5d4b62f3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="https://i.taixr.com/logo.jpeg" alt="Taixr, all about AI &amp; AR" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="https://i.taixr.com/logo.jpeg" alt="Taixr, all about AI &amp; AR" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Taixr</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/docs/category/generative-ai">Tutorial</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div><a href="https://github.com/taixr" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/docs/category/generative-ai">Generative AI</a><button aria-label="Collapse sidebar category &#x27;Generative AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/generative-ai/intro">第一章 多媒体生成入门</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/generative-ai/transformers">第二章 Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/generative-ai/diffusion-models">第三章 扩散模型</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/docs/generative-ai/stable-diffusion">第四章 Stable Diffusion</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/docs/category/learn-javascript-with-p5js">Learn JavaScript with p5.js</a><button aria-label="Expand sidebar category &#x27;Learn JavaScript with p5.js&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/docs/category/python-scripting">Python Scripting</a><button aria-label="Expand sidebar category &#x27;Python Scripting&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/docs/category/generative-ai"><span itemprop="name">Generative AI</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">第四章 Stable Diffusion</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>第四章 Stable Diffusion</h1>
<p>在前一章中，我们介绍了扩散模型及其迭代优化的基本思想。学完该章，我们已经能够生成图像，但训练模型非常耗时，而且我们无法控制生成的图像。在本章中，我们将学习如何从这一阶段走向基于文本条件的模型，这些模型可以根据文本描述高效地生成图像，研究的是一个名为Stable Diffusion（SD）的模型。不过在介绍SD之前，我们会先了解条件模型如何工作，并回顾一些产生当今文生图像模型的创新。</p>
<p>本文相关代码请见<a href="https://github.com/alanhou/ai/blob/master/hands-on-generative-ai/04-stable-diffusion.ipynb" target="_blank" rel="noopener noreferrer">GitHub仓库</a>。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="增强控制条件扩散模型">增强控制：条件扩散模型<a href="#增强控制条件扩散模型" class="hash-link" aria-label="Direct link to 增强控制：条件扩散模型" title="Direct link to 增强控制：条件扩散模型">​</a></h2>
<p>在解决从文本描述生成图像这个具有挑战性的任务之前，先聚焦于一个略简单一点的任务上。我们将了解如何引导我们的模型输出特定类型或类别的 图像。可以使用一种称为条件化的方法，其思想是要求模型生成的不是宽泛的图像，而是属于预定义类别的图像。</p>
<p>模型条件化是一个简单但有效的想法。我们将通过上一章使用的扩散模型，仅做一些小改动。首先，不再使用蝴蝶数据集，而是切换到一个有类别的数据集。我们使用Fashion MNIST，这是一个包含成千上万张衣服图像的数据集，每张图像都带有一个来自10个不同类别的标签。然后是关键，我们将通过模型运行两个输入。不仅向模型展示真实的图像，还会告诉它每张图像所属的类别。我们期望模型学会关联图像和标签，以理解毛衣、靴子等的自有特征。</p>
<p>请注意，我们并不想解决分类问题——不是希望模型告诉我们图像属于哪个类别。我们仍然希望它执行与上一章相同的任务：生成看起来像来自这个数据集的图像。唯一的区别是给了它关于这些图像的附加信息。我们将使用相同的损失函数和训练策略，因为任务是相同的。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="准备数据">准备数据<a href="#准备数据" class="hash-link" aria-label="Direct link to 准备数据" title="Direct link to 准备数据">​</a></h3>
<p>需要一个具有不同图像组的数据集。适用于计算机视觉分类任务的数据集是理想的选择。我们可以从类似ImageNet的数据集开始，该数据集包含数百万张涵盖1000个类别的图像。然而，在这个数据集上训练模型将花费极长的时间。在处理新问题时，最好先从较小的数据集入手，以确保一切按预期进展。这可以缩短反馈环，使我们能够快速迭代并确保方向正确。</p>
<p>我们可以选择MNIST作为这个例子，像上一章那样。为略显不同，我们将选择Fashion MNIST。Fashion MNIST由Zalando开发并开源，是MNIST的替代品，具有类似的特征：压缩大小、黑白图像和十个类别。主要区别在于类别对应于不同类型的衣物，而不是数字，且图像比简单的手写数字包含更多细节。</p>
<p>下面来看一些例子。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from datasets import load_dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from utils.utils import show_images</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fashion_mnist = load_dataset(&quot;fashion_mnist&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">clothes = fashion_mnist[&quot;train&quot;][&quot;image&quot;][:8]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">classes = fashion_mnist[&quot;train&quot;][&quot;label&quot;][:8]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images(clothes, titles=classes, figsize=(4, 2.5))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061001031218.png" alt="" class="img_ev3q"></p>
<p>因此，类别<code>0</code>对应于T恤，类别<code>2</code>是毛衣，而类别<code>9</code>是靴子（Fashion MNIST的<a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist" target="_blank" rel="noopener noreferrer">十个类别</a>）。我们准备数据集和数据加载器的方式与上一章相似，主要的区别是类别信息也作为输入。与上一章的调整大小操作不同，这次我们会将图像输入（<code>28 × 28</code>像素）填充到<code>32 × 32</code>像素。这将保持原始图像的质量，有助于UNet做出更高质量的预测。</p>
<p>注：上一章中图像非常大(<code>512,283</code>)，我们得将其缩放至更小的尺寸。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">import torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from torchvision import transforms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">preprocess = transforms.Compose(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms.ToTensor(),  # Convert to tensor (0, 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms.Pad(2),  # Add 2 pixels on all sides</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def transform(examples):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    images = [preprocess(image) for image in examples[&quot;image&quot;]]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return {&quot;images&quot;: images, &quot;labels&quot;: examples[&quot;label&quot;]}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_dataset = fashion_mnist[&quot;train&quot;].with_transform(transform)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_dataloader = torch.utils.data.DataLoader(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    train_dataset, batch_size=256, shuffle=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="创建类别条件模型">创建类别条件模型<a href="#创建类别条件模型" class="hash-link" aria-label="Direct link to 创建类别条件模型" title="Direct link to 创建类别条件模型">​</a></h3>
<p>diffusers库中的<code>UNet</code>允许提供自定义条件信息。在这里，我们创建一个与上一章中使用的模型类似的模型，但在<code>UNet</code>构造函数中添加了一个<code>num_class_embeds</code>参数。这个参数告诉模型我们希望使用类别标签作为额外条件。我们将使用10，因为这是Fashion MNIST中的类别数。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from diffusers import UNet2DModel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model = UNet2DModel(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    in_channels=1,  # 1 channel for grayscale images</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    out_channels=1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sample_size=32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    block_out_channels=(32, 64, 128, 256),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_class_embeds=10,  # Enable class conditioning</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>要使用这个模型进行预测，我们必须将类别标签作为额外输入传递给<code>forward()</code>方法：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">x = torch.randn((1, 1, 32, 32))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    out = model(x, timestep=7, class_labels=torch.tensor([2])).sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">out.shape</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">torch.Size([1, 1, 32, 32])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>注</strong>：我们还将另一样参数作为条件传递给模型： 时间步！没错，即使是上一章中的模型也可以视作一个条件扩散模型。根据时间步对其进行条件化，期望了解我们在扩散过程中的进展程度将有助于生成更真实的图像。</p>
<p>在内部，时间步和类别标签会被转换为模型在前向传播过程中使用的嵌入。在UNet的多个阶段，这些嵌入会被投射到与给定层中的通道数匹配的维度。然后，这些嵌入会被添加到该层的输出中。这意味着条件信息会被传递到UNet的每个块中，给模型充分的机会来学习如何有效地使用它。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="训练模型">训练模型<a href="#训练模型" class="hash-link" aria-label="Direct link to 训练模型" title="Direct link to 训练模型">​</a></h3>
<p>在灰度图像上添加噪声与上一章的蝴蝶图像效果一样好。来看看在更多噪声时间步时噪声的影响。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from diffusers import DDPMScheduler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scheduler = DDPMScheduler(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">timesteps = torch.linspace(0, 999, 8).long()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">batch = next(iter(train_dataloader))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">x = batch[&quot;images&quot;][0].expand([8, 1, 32, 32])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">noise = torch.rand_like(x)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">noised_x = scheduler.add_noise(x, noise, timesteps)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images((noised_x * 0.5 + 0.5).clip(0, 1))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061001120592.png" alt="" class="img_ev3q"></p>
<p>我们的训练方式也几乎与上一章相同，只是现在传递类别标签作为条件。注意，这只是为模型提供的附加信息，并不影响定义损失函数的方式。开启训练，可以泡杯茶、咖啡或其他饮料。</p>
<blockquote>
<p><strong>注</strong>：我们还将使用Python包tqdm在训练过程中显示进度。作者忍不住分享他们文档中的这句话（<a href="https://tqdm.github.io" target="_blank" rel="noopener noreferrer">https://tqdm.github.io</a>）：</p>
<p>tqdm在阿拉伯语中意为“进展”（taqadum, تقدّم），在西班牙语中是“我非常爱你”（te quiero demasiado）的缩写。</p>
</blockquote>
<p>不要被下面的代码吓到：它与我们进行无条件生成时的类似（建议将这段代码与上一章的代码进行对比。你能找到所有的不同之处吗？）。</p>
<ol>
<li>加载一批图像及其相应的标签。</li>
<li>根据时间步为图像添加噪声。</li>
<li>将带噪声的图像和类别标签喂给模型。</li>
<li>计算损失。</li>
<li>反向传播损失并用优化器更新模型权重。</li>
</ol>
<p>注：epoch和学习率的数量不同、<code>AdamW</code>的epsilon不一样，还使用了_tqdm加载数据、标签并将标签传递给模型。最重要的是条件为2-line diff。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from torch.nn import functional as F</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from tqdm import tqdm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scheduler = DDPMScheduler(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">num_epochs = 25</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">lr = 3e-4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">losses = []  # Somewhere to store the loss values for later plotting</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model = model.to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Train the model (this takes a while!)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for epoch in (progress := tqdm(range(num_epochs))):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for step, batch in (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        inner := tqdm(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            enumerate(train_dataloader),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            position=0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            leave=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            total=len(train_dataloader),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Load the input images and classes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        clean_images = batch[&quot;images&quot;].to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        class_labels = batch[&quot;labels&quot;].to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample noise to add to the images</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise = torch.randn(clean_images.shape).to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample a random timestep for each image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        timesteps = torch.randint(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            scheduler.config.num_train_timesteps,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            (clean_images.shape[0],),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            device=device,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ).long()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Add noise to the clean images according to the timestep</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get the model prediction for the noise - note the use of class_labels</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise_pred = model(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            noisy_images,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            timesteps,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            class_labels=class_labels,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return_dict=False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compare the prediction with the actual noise:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss = F.mse_loss(noise_pred, noise)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Display loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        inner.set_postfix(loss=f&quot;{loss.cpu().item():.3f}&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Store the loss for later plotting</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        losses.append(loss.item())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update the model parameters with the optimizer based on this loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss.backward(loss)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer.step()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer.zero_grad()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">import matplotlib.pyplot as plt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">plt.plot(losses)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061007525731.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="采样">采样<a href="#采样" class="hash-link" aria-label="Direct link to 采样" title="Direct link to 采样">​</a></h3>
<p>现在有了一个在做预测时需要两个输入即图像和类别标签的模型。可以通过从随机噪声开始，然后逐步去噪，传入我们想生成的类别标签来创建样本：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def generate_from_class(class_to_generate, n_samples=8):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sample = torch.randn(n_samples, 1, 32, 32).to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    class_labels = [class_to_generate] * n_samples</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    class_labels = torch.tensor(class_labels).to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for _, t in tqdm(enumerate(scheduler.timesteps)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get model pred</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            noise_pred = model(sample, t, class_labels=class_labels).sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update sample with step</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        sample = scheduler.step(noise_pred, t, sample).prev_sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return sample.clip(-1, 1) * 0.5 + 0.5</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Generate t-shirts (class 0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">images = generate_from_class(0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images(images, nrows=2)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1000it [00:13, 75.05it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061007545876.png" alt="T-shirts" class="img_ev3q"></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Now generate some sneakers (class 7)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">images = generate_from_class(7)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images(images, nrows=2)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1000it [00:13, 76.91it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061007560034.png" alt="sneakers" class="img_ev3q"></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># ...or boots (class 9)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">images = generate_from_class(9)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images(images, nrows=2)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1000it [00:13, 76.29it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061007564932.png" alt="boots" class="img_ev3q"></p>
<p>可以看到，生成的图像远称不上完美。如果进一步探索架构并延长训练时间，图像质量将大大提升。但令人惊奇的是，模型仅通过对训练数据发送这些信息，就学习到了不同类型衣物的形状，并意识到形状<code>9</code>与形状<code>0</code>不同。换句话说，模型习惯于看到数字<code>9</code>与靴子一起出现。当我们要求它生成图像并提供数字9时，它会生成一双靴子。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="提高效率潜在扩散">提高效率：潜在扩散<a href="#提高效率潜在扩散" class="hash-link" aria-label="Direct link to 提高效率：潜  在扩散" title="Direct link to 提高效率：潜在扩散">​</a></h2>
<p>既然可以训练一个条件模型，我们需要扩展它并将条件从类别标签改为文本，对吧？……对吗？实际并非如此。随着图像大小的增加，处理这些图像所需的算力也随之增加。这在自注意力机制中尤为明显，其中运算量按输入数量的平方增长。一个128像素的正方形图像的像素量是64像素正方形图像的四倍，在自注意力层中需要16倍的内存和运算。这对所有希望生成高分辨率图像的人来说都是个问题。</p>
<p>潜在扩散尝试通过使用一个独立的变分自编码器来缓解这一问题。正如我们在第二章中所见，VAE可以将图像压缩到更小的空间维度。其原理是图像往往包含大量冗余信息。给定足够的训练数据，VAE可以学习生成输入图像的更小表示，然后基于这个小的潜在表示高保真地重建图像。Stable Diffusion中使用的VAE接收三通道图像，并生成四通道的潜在表示，每个空间维度的缩减因子为8。一个512像素的正方形输入图像（3x512x512=786,432个值）将被压缩为4x64x64的潜在表示（16,384个值）。</p>
<p>通过对这些较小的潜在表示进行扩散处理，而非对全分辨率图像进行扩散，我们可以获得使用较小图像的许多好处（较低的内存使用量，UNet所需的层数较少，更快的生成时间等），并且在准备查看时仍然可以将结果解码回高分辨率图像。这一创新大大降低了训练和运行这些模型的成本。介绍这一思想的论文《<a href="http://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">潜在扩散模型</a>》通过训练基于分割图、类别标签和文本条件的模型，展示了这一技术的威力。优秀的结果促成了作者与RunwayML、LAION、StabilityAI和EleutherAI等合作伙伴的进一步合作，训练出更强大的模型版本，即Stable Diffusion。</p>
<p>注 ：LAION和EleutherAI是聚焦于开放机器学习的非营利性组织。StabilityAI是一家开放访问机器学习的前沿公司。RunwayML是构建针对创意应用AI驱动工具的公司。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stable-diffusion深入了解组件">Stable Diffusion：深入了解组件<a href="#stable-diffusion深入了解组件" class="hash-link" aria-label="Direct link to Stable Diffusion：深入了解组件" title="Direct link to Stable Diffusion：深入了解组件">​</a></h2>
<p>Stable Diffusion是一个基于文本条件的潜在扩散模型。由于其广受欢迎，数百个网站和应用程序上无需任何技术知识即可使用它来创建图像。它也得到了诸如diffusers等库的良好支持，这些库让我们可以使用用户友好的管道来生成Stable Diffusion图像：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from diffusers import AutoencoderKL, StableDiffusionPipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vae = AutoencoderKL.from_pretrained(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;stabilityai/sd-vae-ft-ema&quot;, torch_dtype=torch.float16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pipe = StableDiffusionPipeline.from_pretrained(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;runwayml/stable-diffusion-v1-5&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    vae=vae,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    torch_dtype=torch.float16,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    variant=&quot;fp16&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).to(device)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pipe(&quot;Watercolor illustration of a rose&quot;).images[0]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  0%|          | 0/50 [00:00&lt;?, ?it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/202406100805539.png" alt="Watercolor illustration of a rose" class="img_ev3q"></p>
<p>本节将探讨驱动这一切的所有组件。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="文本编码器">文本编码器<a href="#文本编码器" class="hash-link" aria-label="Direct link to 文本编码器" title="Direct link to 文本编码器">​</a></h3>
<p>那么，Stable Diffusion是如何理解文本的呢？前面展示了如何通过向UNet提供附加信息，使我们能够对生成的图像类型进行一定的控制。给定一个图像的噪声版本，模型的任务是根据附加线索（如类别标签）预测去噪版本。对SD，附加线索是文本提示词。在推理时，我们输入想要生成的图像描述和一些纯噪  声作为初始点，模型会尽力将随机输入去噪为与标题匹配的图像。</p>
<p>为实现这一点，我们需要创建一个能够捕捉文本描述相关信息的数字表示。为此，我们使用一个文本编码器，它将输入字符串转换为文本嵌入，这些嵌入会与时间步和噪声潜在变量一起输入到UNet中。</p>
<p><img decoding="async" loading="lazy" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098149239/files/assets/simplified_unet.png" alt="简化的unet" class="img_ev3q"></p>
<p>为此，SD利用了一个基于CLIP的预训练transformer模型，该模型在<a href="https://alanhou.org/ai-transformers/" target="_blank" rel="noopener noreferrer">第二章</a>中介绍。文本编码器是一个transformer模型，它接收一个令牌序列，并为每个令牌生成一个向量。在Stable Diffusion的早期版本（SD 1到1.5）中，使用了OpenAI的原始CLIP，文本编码器映射到一个768维的向量。由于CLIP的原始数据集未知，社区训练了一个名为OpenCLIP的开源版本，它基于来自LAION非营利组织的数据集进行训练。Stable Diffusion 2使用了OpenCLIP的文本编码器，它为每个令牌生成1024维的向量。</p>
<p>我们不将所有令牌的向量合并成单一表示，而是将它们分开并用作UNet的条件。这使得UNet可以分别使用每个令牌中的信息，而不仅仅是提示词的整体含义。由于我们是从CLIP模型的内部表示中提取这些文本嵌入，它们通常被称为编码器隐藏态。我们来深入了解文本编码器的工作原理。这与我们在第三章中看到的编码器模型的流程相同。</p>
<p><img decoding="async" loading="lazy" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098149239/files/assets/text_encoder.png" alt="文本编码器图示" class="img_ev3q"></p>
<p>该图展示了文本编码流程，它将输入的提示词转换为一组文本嵌入（编码器隐藏状态），这些嵌入可以作为条  件输入到UNet中。</p>
<p>编码文本的第一步是执行标记化的。这将字符序列转换为数字序列，每个数字代表一组不同的字符。在下面的示例中，我们可以看到Stable Diffusion的分词器如何对一个短语进行标记。提示词中的每个令牌都被分配了一个唯一的令牌编号（例如，“<code>photograph</code>”在标记器的词汇表中恰好是8853）。还有一些额外的令牌提供附加的上下文信息，例如句子结束的位置。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">prompt = &quot;A photograph of a puppy&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Turn the text into a sequence of tokens:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text_input = pipe.tokenizer(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prompt,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    padding=&quot;max_length&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_length=pipe.tokenizer.model_max_length,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    truncation=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return_tensors=&quot;pt&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># See the individual tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for t in text_input[&quot;input_ids&quot;][0][:8]:  # We&#x27;ll just look at the first 7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    print(t, pipe.tokenizer.decoder.get(int(t)))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">tensor(49406) &lt;|startoftext|&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(320) a&lt;/w&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(8853) photograph&lt;/w&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(539) of&lt;/w&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(320) a&lt;/w&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(6829) puppy&lt;/w&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(49407) &lt;|endoftext|&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tensor(49407) &lt;|endoftext|&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>文本在标记化后，可以将其传递给文本编码器，以获得最终的文本嵌入，这些嵌入将被喂给UNet：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">text_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Text embeddings shape:&quot;, text_embeddings.shape)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Text embeddings shape: torch.Size([1, 77, 768])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="无分类器引导">无分类器引导<a href="#无分类器引导" class="hash-link" aria-label="Direct link to 无分类器引导" title="Direct link to 无分类器引导">​</a></h3>
<p>尽管做了很多努力来使文本条件尽可能有用，但模型在预测时仍然倾向于主要依赖于噪声输入图像而不是提示词。在某种程度上，这可以理解——许多标题与其关联的图像关系不大，因此模型学会了不要过分依赖描述。然而，在生成新图像时，这种情况是不可取的——如果模型不遵循提示词，我们可能会得到与描述不相关的图像。</p>
<p>为解决这一问题，我们引入了引导（<em>guidance</em>）。引导通常是指所有提供更多控制采样处理的方法。一种可行的选择是修改损失函数以偏向特定方向。例如，如果我们想让生成的图像偏向特定颜色，可以改变损失函数来衡量我们与目标颜色的平均距离。另一种选择是使用诸如CLIP或分类器等模型来评估结果，并将它们的损失信号作为生成过程的一部分。例如，使用CLIP，我们可以比较提示文本与生成图像嵌入之间的差异，并引导扩散过程最小化这一差异。在练习部分将展示如何使用这种技术。</p>
<p>另一种选择是使用一种称为无分类器引导（Classifier-Free Guidance，CFG）的技巧，它结合了有条件和无条件扩散模型的生成。在训练过程中，有时会将文本条件留空，迫使模型学习在没有任何文本信息的情况下对图像进行去噪（无条件生成）。然后，在推理时我们做出两个预测：一个使用文本提示词作为条件，另一个不使用。然后，可以利用这两个预测之间的差异来创建一个最终的组合预测，根据某个缩放因子（引导比例）进一步向文本条件预测所指示的方向推进，希望最终生成的图像更符合提示词。为了引入引导，我们可以通过<code>noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</code>等修改噪声预测。这一小改动效果出奇地好，使我们能够更好地控制生成过程。在本章稍后将深入探讨实现细节，但先来看看如何使用它：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">images = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt = &quot;An oil painting of a collie in a top hat&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for guidance_scale in [1, 2, 4, 12]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    torch.manual_seed(0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image = pipe(prompt, guidance_scale=guidance_scale).images[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    images.append(image)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  0%|          | 0/50 [00:00&lt;?, ?it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  0%|          | 0/50 [00:00&lt;?, ?it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  0%|          | 0/50 [00:00&lt;?, ?it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  0%|          | 0/50 [00:00&lt;?, ?it/s]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from utils.utils import image_grid</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image_grid(images, 1, 4)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061008151738.jpg" alt="An oil painting of a collie in a top hat" class="img_ev3q"></p>
<p>由提示词“<em><code>An oil painting of a collie in a top hat</code></em>”生成的图像，CFG比例从左到右分别为1、2、4和12</p>
<p>可以看出，更高的值会生成更符合描述的图像，但如果设置得太高，可能会让图像过饱和。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vae变分自编码器">VAE（变分自编码器）<a href="#vae变分自编码器" class="hash-link" aria-label="Direct link to VAE（变分自编码器）" title="Direct link to VAE（变分自编码器）">​</a></h3>
<p>VAE的任务是将图像压缩为更小的潜在表示并再次重建。用于Stable Diffusion的VAE是一个非常出色的模型。我们在这里不会讨论训练细节，但除了第二章中描述的常规重建损失和KL散度外，VAE还使用了额外的基于补丁的鉴别器损失来帮助模型学习生成合理的细节和纹理。这为训练增加了类似GAN的组件，有助于避免以前VAE模型中常见的稍微模糊的输出。像文本编码器一样，VAE通常是单独训练的，并在扩散模型训练和采样过程中作为frozen组件使用。</p>
<p><img decoding="async" loading="lazy" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098149239/files/assets/vae.png" alt="" class="img_ev3q"></p>
<p>图4-1 VAE架构</p>
<p>我们来加载一张图像，看看它经过VAE压缩和解压后的样子：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from utils.utils import load_image, show_image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">im = load_image(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;https://huggingface.co/datasets/genaibook/images/resolve/main/llama.jpeg&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    size=(512, 512),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_image(im)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061008190268.png" alt="llama" class="img_ev3q"></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tensor_im = transforms.ToTensor()(im).unsqueeze(0).to(device) * 2 - 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latent = vae.encode(tensor_im.half())  # Encode the image to a distribution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latents = latent.latent_dist.sample()  # Sampling from the distribution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # This scaling factor was introduced by the SD authors to reduce the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # variance of the latents. Can be accessed via vae.config.scaling_factor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latents = latents * 0.18215</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">latents.shape</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">torch.Size([1, 4, 64, 64])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Plot the individual channels of the latent representation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_images(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [l for l in latents[0]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    titles=[f&quot;Channel {i}&quot; for i in range(latents.shape[1])],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ncols=4,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061008203653.png" alt="Plot the individual channels of the latent representation" class="img_ev3q"></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image = vae.decode(latents / 0.18215).sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = (image / 2 + 0.5).clamp(0, 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_image(image[0].float())</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061008213381.png" alt="llama" class="img_ev3q"></p>
<p>从零开始生成图像时，我们会先创建一组随机的潜在表示。通过迭代地优化这些噪声潜在表示来生成样本，然后使用VAE解码器将这些最终的潜在表示解码为可以查看的图像。只有在希望从现有图像开始处理时，才会使用编码器，这将在第7章探讨。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="unet">UNet<a href="#unet" class="hash-link" aria-label="Direct link to UNet" title="Direct link to UNet">​</a></h3>
<p>用于Stable Diffusion的UNet与上一章中用于生成图像的UNet类似。不同的是，它的输入不是一个三通道图像，而是一个四通道的潜在表示。时间步嵌入的输入方式与本章开始例子中的类别条件输入方式相同。但这个UNet还需要接收文本嵌入作为额外的条件。在UNet中散布着交叉注意力层，UNet中的每个空间位置都可以关注文本条件中的不同标记，从提示中引入相关信息。下图展示了文本条件（以及基于时间步的条件）在不同点的输入方式。</p>
<p><img decoding="async" loading="lazy" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098149239/files/assets/sd_unet.png" alt="" class="img_ev3q"></p>
<p>图4-2 UNet图示</p>
<p>Stable Diffusion版本1和2的UNet大约有8.6亿参数。最新的<a href="http://arxiv.org/abs/2307.01952" target="_blank" rel="noopener noreferrer">Stable Diffusion XL（SDXL）</a>中的UNet参数更多，大约是26亿，并且使用了额外的条件信息。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stable-diffusion-xl">Stable Diffusion XL<a href="#stable-diffusion-xl" class="hash-link" aria-label="Direct link to Stable Diffusion XL" title="Direct link to Stable Diffusion XL">​</a></h3>
<p>2023年夏天，发布了一个更好的Stable Diffusion版本：Stable Diffusion XL。它使用了本章描述的相同原理，并对所有系统组件进行了各种改进。一些最令人兴奋的变化有：</p>
<ul>
<li>
<p>更大的文本编码器可捕捉更好的提示词表示。它使用了两个文本编码器的输出并将这些表示连接起来。</p>
</li>
<li>
<p>对一切进行条件化。除了携带噪声量信息的时间步和文本嵌入外，SDXL还使用了以下额外的条件信号：</p>
<ul>
<li>原始图像大小。训练集中不再丢弃小图像（占用于训练SDXL的总训练数据的近40%），而是将小图像放大并在训练中使用。不过，模型也会接收关于图像大小的信息，从而学到放大伪影（upscaling artifacts）不应出现在大图像中，并鼓励在推理过程中产生更好的质量。</li>
<li>裁剪坐标。在训练过程中，输入图像通常会被随机裁剪，因为批次中的所有图像必须具有相同的大小。随机裁剪可能会产生不良影响，例如切掉主体的头部或完全移除图像中的主体，虽然可能在文本提示词中有相应描述。在模型训练完成后，如果我们请求未裁剪的图像（通过将裁剪坐标设置为<code>(0, 0)</code>），模型更有可能生成居中的主体。</li>
<li>目标宽高比。在对方形图像进行初步预训练后，SDXL在各种宽高比上进行了微调，并将原始宽高比信息作为另一个条件信号使用。与其他条件情况一样，这使得生成的风景和肖像图像比以前更为真实且伪影更少。</li>
</ul>
</li>
<li>
<p>更大的分辨率。SDXL设计用于生成<code>1024×1024</code>像  素分辨率的图像（或像素总数约为1024^2的非方形图像）。和宽高比一样，这一特性是在微调阶段实现的。</p>
</li>
<li>
<p>UNet的规模大约是原来的三倍。交叉注意力上下文变大，以适应更多的条件。</p>
</li>
<li>
<p>改进的VAE。它使用与原始Stable Diffusion相同的架构，但在更大的批次上进行训练，并使用EMA（指数移动平均）技术来更新权重。</p>
</li>
<li>
<p>更优的模型。除了基础模型外，SDXL还包括一个额外的优化模型，该模型在与基础模型相同的潜在空间上。但该模型仅在噪声调度的前20%期间在高质量图像上训练。这意味着它知道如何将带有少量噪声的图像转换为高质量的纹理和细节。</p>
</li>
</ul>
<p>由于原始Stable Diffusion是开源的，其他研究人员和开源社区已经探索了许多这类技术。SDXL结合了这些想法，实现了图像质量的显著提升，但运行模型的速度较慢且占用更多内存。我们的主要收获是，我们讨论的原则（特别是条件化）是引导生成模型行为的优秀通用工具，而开源发布可以加速探索。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="博采众长注释采样循环">博采众长：注释采样循环<a href="#博采众长注释采样循环" class="hash-link" aria-label="Direct link to 博采众长：注释采样循环" title="Direct link to 博采众长：注释采样循环">​</a></h3>
<p>现在我们知道每个组件的作用了，下面将它们结合起来生成图像，不再依赖于管道。以下是我们要使用的配置：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Some settings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompt = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Acrylic palette knife painting of a flower&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]  # What we want to generate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">height = 512  # default height of Stable Diffusion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">width = 512  # default width of Stable Diffusion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">num_inference_steps = 30  # Number of denoising steps</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">guidance_scale = 7.5  # Scale for classifier-free guidance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">seed = 42  # Seed for random number generator</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>第一步是对文本提示进行编码。由于计划进行无分类器引导，我们将创建两组文本嵌入：一组是提示词嵌入，另一组代表空字符串，即无条件输入。尽管这里我们会使用无条件输入，这一配置提供了很大的灵活性。例如，我们可以：</p>
<ul>
<li>编码反向提示词替换空字符串。添加反向提示词可引导模型避免朝某个方向生成。在本章的练习6中，读者将会尝试使用反向提示词。</li>
<li>组合多个不同权重的提示词。提示词权重让我们可以强化或弱化提示词的某些部分。</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Tokenize the input</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text_input = pipe.tokenizer(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prompt,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    padding=&quot;max_length&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_length=pipe.tokenizer.model_max_length,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    truncation=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return_tensors=&quot;pt&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Do the same for the unconditional input (a blank string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">uncond_input = pipe.tokenizer(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    padding=&quot;max_length&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_length=pipe.tokenizer.model_max_length,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return_tensors=&quot;pt&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Feed both embeddings through the text encoder</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    text_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(device))[0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Concatenate the two sets of text embeddings embeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>接下来，我们创建随机初始潜在表示并配置调度器来使用期望的推理步数：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Prepare the Scheduler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pipe.scheduler.set_timesteps(num_inference_steps)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Prepare the random starting latents</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">latents = (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    torch.randn(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        (1, pipe.unet.config.in_channels, height // 8, width // 8),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    .to(device)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    .half()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">latents = latents * pipe.scheduler.init_noise_sigma</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>然后遍历采样步骤，获取每个阶段的模型预测并用其更新潜在模型：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">for i, t in enumerate(pipe.scheduler.timesteps):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Create two copies of the latents to match the two text embeddings (unconditional and conditional)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latent_model_input = torch.cat([latents] * 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Predict the noise residual for both sets of inputs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise_pred = pipe.unet(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            latent_model_input, t, encoder_hidden_states=text_embeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ).sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Split the prediction into unconditional and conditional versions:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Perform classifier-free guidance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    noise_pred = noise_pred_uncond + guidance_scale * (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise_pred_text - noise_pred_uncond</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Compute the previous noisy sample x_t -&gt; x_t-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>注意无分类器引导步骤。我们的最终噪声预测是 <code>noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</code>，将预测从无条件预测推向基于提示的预测。试着改变引导比例，看下输出会受何影响。</p>
<p>到循环结束时，潜在表示应该会展现一个符合提示词的图像。最后一步是使用VAE将潜在表示解码成图像，以便我们看到结果：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Scale and decode the image latents with the VAE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">latents = 1 / vae.config.scaling_factor * latents</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">with torch.no_grad():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image = vae.decode(latents).sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = (image / 2 + 0.5).clamp(0, 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">show_image(image[0].float())</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="https://i.cdnl.ink/homepage/wp-content/uploads/2024/06/2024061008264896.png" alt="Acrylic palette knife painting of a flower" class="img_ev3q"></p>
<p>如果阅读<code>StableDiffusionPipeline</code>的源代码，会发现以上代码与管道使用的<code>call()</code>方法非常相似。希望注释版本能展示幕后并没有什么神奇的事情。在遇到添加了各种技巧的其他管道时，可以将其作为参考。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="开放数据开放模型">开放数据，开放模型<a href="#开放数据开放模型" class="hash-link" aria-label="Direct link to 开放数据，开放模型" title="Direct link to 开放数据，开放模型">​</a></h2>
<p><a href="https://laion.ai/blog/laion-5b/" target="_blank" rel="noopener noreferrer">LAION-5B数据集</a>包含超过50亿个图像URL及其相应的描述（图像-描述对）。这个数据集首先从CommonCrawl（一个类似Google搜索引擎索引互联网的开放网络爬取数据存储库）中获取所有图像URL，然后使用CLIP只保留文本与图像高度相似的图像-描述对。</p>
<p>这个数据集由开源ML社区创建，旨在满足开放访问此类数据集的需求。在LAION计划之前，只有少数大型公司的研究实验室能够获取图像-文本对数据集。这些组织将数据集的详细信息保密，使其结果无法验证或复制。通过创建一个公开可用的URL和描述索引源，LAION使得许多小型社区和组织能够训练模型并进行研究，这在以前是不可想象的。</p>
<p>潜在扩散模型就是这样一种模型，由CompVis使用4亿个图像-文本对的旧版LAION数据集训练。基于LAION训练的潜在扩散模型的发布首次为整个研究社区提供了强大的文生图模型。</p>
<p>注：CompVis当前是海德堡大学的计算机视觉小组，为LMU Munich的研究小组 <a href="https://github.com/CompVis" target="_blank" rel="noopener noreferrer"><em>https://github.com/CompVis</em></a></p>
<p>潜在扩散的成功展示了这种方法的潜力，在后续产品Stable Diffusion中得到了实现，它是CompVis与当时两家初创公司Stability AI和Runway ML的合作。训练类似SD的模型需要大量的GPU时间。即使利用免费的LAION数据集，也只有少数人能够承担GPU小时的投入。这就是为什么模型权重和代码的公开发布如此重要——它首次提供了一个功能强大的文生图模型，具有与最佳闭源产品类似的能力。</p>
<p>Stable Diffusion的公开可用性使其成为过去几年中研究人员和开发人员探索这一技术的首选。数百篇论文在基础模型上进行构建，添加了新功能或找到了改进速度和质量的创新方法。除了研究论文之外，一个不一定来自机器学习背景的多样化社区一直在使用这些模型进行新创意工作流的探索，优化以实现更快的推理等等。无数初创公司已经找到了将这些快速改进的工具整合到其产品中的方法，形成了一个新的应用生态系统。</p>
<p><img decoding="async" loading="lazy" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098149239/files/assets/cell-32-output-1.png" alt="01_04_stable_diffusion_files/figure-asciidoctor/cell-32-output-1" class="img_ev3q"></p>
<p>Stable Diffusion发布后的几个月展示了在公开环境中共享这些技术的影响 ，更多进一步的质量改进和定制技术将在第7章和第8章中进行探索。SD的质量与当时的商业产品（如DALL-E和MidJourney）也有的拼，成千上万的人着力使其变得更好，并在这个开源的基础上进行构建。希望这个例子能鼓励其他人效仿并与开源社区分享他们的工作。</p>
<blockquote>
<p><strong>注</strong>：除了用于训练Stable Diffusion，LAION-5B还被许多其他研究项目使用。比如OpenCLIP，这是LAION社区的一项努力，旨在训练高质量（最先进）的开源CLIP模型并复制类似原始模型的质量。高质量的开源CLIP模型对许多任务有益，如图像检索和零样本图像分类。训练模型数据的透明度也使得研究扩大模型规模的影响、正确复现结果以及使研究更易于访问成为可能。</p>
</blockquote>
<p>LAION组织和数据集对推动研究和增强开源社区中的实验具有巨大的影响。然而，基于这些模型的文本到图像生成模型和下游商业应用的巨大成功引发了对这些数据集数据来源的担忧。</p>
<p>因为该数据集包含从互联网上爬取的图像链接，其中包含数百万指向可能包含受版权保护的材料（如照片、艺术作品、漫画、插图等）的URL。研究还发现，这些数据集还包括私人敏感信息，如公开可用的个人医疗影像。</p>
<p>注：2022年在Stable Diffusion于发布后不久出现了相关的文章 - <a href="https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/" target="_blank" rel="noopener noreferrer"><em>https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/</em></a></p>
<p>使用这样的数据集来训练生成式AI模型还可能使模型具有生成内容的能力，这些内容会强化或加剧<a href="http://arxiv.org/abs/2303.11408" target="_blank" rel="noopener noreferrer">社会偏见</a>，并可能用于生成显式成人内容。然而，这些开源模型是基于开源数据集训练的，因此可以<a href="http://arxiv.org/abs/2211.05105" target="_blank" rel="noopener noreferrer">研究、分析和缓解这些偏见和问题内容</a>。</p>
<p>尽管一些国家对于研究用途的版权法有合理使用豁免，其他国家在使用抓取数据来训练机器学习模型方面也有利的先例，但当一个研究模型被用于商业级的大规模生成AI时会发生什么？这个复杂的话题目前正在美国和欧洲不同司法管辖区的法院进行诉讼，涉及的角度包括版权法、研究应用的合理使用、隐私、AI工具对创意工作的经济影响等。对此类复杂问题我们没有答案，但这种法律灰色地带正在推动研究和开源社区远离使用开源数据集；对于Stable Diffusion XL，用于训练的数据集并未披露，尽管模型权重是开源的。</p>
<p>构建一个以同意、安全和许可为中心的新大规模文本-图像数据集也将是研究和开源社区的优质资源，并为下游商业应用提供法律保障。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="项目用gradio构建一个交互式机器学习演示">项目：用Gradio构建一个交互式机器学习演示<a href="#项目用gradio构建一个交互式机器学习演示" class="hash-link" aria-label="Direct link to 项目：用Gradio构建一个交互式机器学习演示" title="Direct link to 项目：用Gradio构建一个交互式机器学习演示">​</a></h2>
<p>截至目前，我们专注于使用开源库运行transformer和扩散模型。这给我们带来了很多灵活性和对模型的控制，但也需要大量工作来配置运行。现实是大多数人不懂编程，但可能对探索模型及其功能感兴趣。</p>
<p>在这个项目中，我们将构建一个简单的机器学习演示，允许用户使用Stable Diffusion根据文本提示词生成图像。演示可轻松地向众多用户展示模型，使我们的工作和研究更易于访问。我们将使用Gradio开源库构建演示，该库可创建简单的web应用并使用Python进行自定义。</p>
<p>Gradio可以在很多地方运行，如Python IDE、Jupyter notebook、Google Colab或云环境如Hugging Face空间等。构建Gradio演示的最简单方法是使用其<code>Interface</code>类，它有三大关键点：</p>
<ul>
<li><code>inputs</code>：演示的预期输入类型，如文本提示词或图像</li>
<li><code>outputs</code>：演示的预期输出类型，如生成的图像</li>
<li><code>fn</code>：用户互动时调用的函数。这是魔法产生的核心。可以在这里运行任意代码，包括使用transformers或diffusers运行模型。</li>
</ul>
<p>来看一个例子：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">import gradio as gr</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def greet(name):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return &quot;Hello &quot; + name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">demo = gr.Interface(fn=greet, inputs=&quot;text&quot;, outputs=&quot;text&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">demo.launch()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># TODO: replace this with an image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Running on local URL:  http://127.0.0.1:7860</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">To create a public link, set `share=True` in `launch()`.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;IPython.core.display.HTML object&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>轮到读者实操了！构建一个简单的演示，让用户可使用Stable Diffusion通过文本提示词生成图像。可以使用前一节的初始代码。演示运行起来后，建议添加更多功能使其更具交互性和趣味性。例如，你可以：</p>
<ul>
<li>添加一个滑块来控制引导比例</li>
<li>添加一个按钮，允许用户上传自己的图像并从中生成新图像</li>
<li>添加一个标题和描述，让用户了解演示的内容</li>
</ul>
<p>如需帮助，请记得查看<a href="https://www.gradio.app/docs/interface" target="_blank" rel="noopener noreferrer">官方文档</a>和<a href="https://www.gradio.app/guides/quickstart" target="_blank" rel="noopener noreferrer">快速入  门指南</a>。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结">总结<a href="#总结" class="hash-link" aria-label="Direct link to 总结" title="Direct link to 总结">​</a></h2>
<p>本章展示了条件控制如何为我们提供新的方式来控制扩散模型生成的图像。我们已经看到了文本编码器如何让扩散模型通过文本提示词进行条件控制，从而实现强大的文生图功能。通过深入研究采样循环，我们探索了所有这些如何在Stable Diffusion模型中组合在一起，并了解了不同组件如何协同工作。</p>
<p>在第7章，读者将学习如何微调Stable Diffusion，为模型添加新知识或能力。例如，通过展示你的宠物的照片，Stable Diffusion可以学习到“<code>your pet</code>”的概念，并在新的场景中生成新的图像，例如“<code>your pet on the moon</code>”。</p>
<p>稍后，在第8章，我们将展示一些可以添加到扩散模型中的功能，使其不仅是简单的图像生成。例如，我们将探索图像修复，可以掩盖图像的一部分然后填充该部分。第8章还探讨了基于提示词编辑图像的技术。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="练习">练习<a href="#练习" class="hash-link" aria-label="Direct link to 练习" title="Direct link to 练习">​</a></h2>
<ol>
<li>
<p>类条件扩散模型的训练过程与非条件模型的训练过程有何不同，特别是在输入数据和使用的损失函数方面？</p>
</li>
<li>
<p>时间步嵌入如何影响扩散过程中的图像质量和演变？</p>
</li>
<li>
<p>说明潜在扩散和普通扩散的区别。使用潜在扩散有哪些利弊？</p>
</li>
<li>
<p>文本提示词是如何融入模型的？</p>
</li>
<li>
<p>基于模型的引导和无分类器引导有什么区别？无分类器引导的好处是什么？</p>
</li>
<li>
<p>使用反向提示词有什么效果？用<code>pipe(…, negative_prompt=““)</code>进行试验。如何能够使用Stable Diffusion来引导图像？</p>
</li>
<li>
<p>假设想从所有生成的图像中去除白色帽子。如何使用反向提示词来实现这？尝试使用高级管道并调整端到端推理示例（提示：这只需要修改无分类器条件的随机部分）。</p>
</li>
<li>
<p>在SDXL中，如果使用<code>(256, 256)</code>而不是<code>(1024, 1024)</code>作为“原始尺寸”条件信号，会发生什么？如果使用<code>(0, 0)</code>以外的裁剪坐标，会发生什么？你能解释为什么吗？<br>
<strong>挑战</strong></p>
</li>
<li>
<p><strong>蓝色引导</strong>。假设我们希望生成的图像偏向特定颜色，例如蓝色。如何做到这一点？第一步是定义一个我们想要最小化的条件函数，本例中是一个颜色损失。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def color_loss(images, target_color=(0.1, 0.5, 0.9)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;&quot;&quot;Given a target color (R, G, B) return a loss for how far away on average</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    the images&#x27; pixels are from that color.&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    target = (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        torch.tensor(target_color).to(images.device) * 2 - 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )  # Map target color to (-1, 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    target = target[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        None, :, None, None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]  # Get shape right to work with the images (b, c, h, w)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    error = torch.abs(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        images - target</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ).mean()  # Mean absolute difference between the image pixels and the target color</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return error</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>根据这一损失函数，编写一个采样循环（无需训练），修改损失函数中的<code>x</code>。为简化工具，推荐使用上一章中的无条件<code>DDPMPipeline</code>。</p>
</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/taixr/portal/tree/main/docs/generative-ai/stable-diffusion.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/docs/generative-ai/diffusion-models"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">第三章 扩散模型</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/docs/category/learn-javascript-with-p5js"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Learn JavaScript with p5.js</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#增强控制条件扩散模型" class="table-of-contents__link toc-highlight">增强控制：条件扩散模型</a><ul><li><a href="#准备数据" class="table-of-contents__link toc-highlight">准备数据</a></li><li><a href="#创建类别条件模型" class="table-of-contents__link toc-highlight">创建类别条件模型</a></li><li><a href="#训练模型" class="table-of-contents__link toc-highlight">训练模型</a></li><li><a href="#采样" class="table-of-contents__link toc-highlight">采样</a></li></ul></li><li><a href="#提高效率潜在扩散" class="table-of-contents__link toc-highlight">提高效率：潜在扩散</a></li><li><a href="#stable-diffusion深入了解组件" class="table-of-contents__link toc-highlight">Stable Diffusion：深入了解组件</a><ul><li><a href="#文本编码器" class="table-of-contents__link toc-highlight">文本编码器</a></li><li><a href="#无分类器引导" class="table-of-contents__link toc-highlight">无分类器引导</a></li><li><a href="#vae变分自编码器" class="table-of-contents__link toc-highlight">VAE（变分自编码器）</a></li><li><a href="#unet" class="table-of-contents__link toc-highlight">UNet</a></li><li><a href="#stable-diffusion-xl" class="table-of-contents__link toc-highlight">Stable Diffusion XL</a></li><li><a href="#博采众长注释采样循环" class="table-of-contents__link toc-highlight">博采众长：注释采样循环</a></li></ul></li><li><a href="#开放数据开放模型" class="table-of-contents__link toc-highlight">开放数据，开放模型</a></li><li><a href="#项目用gradio构建一个交互式机器学习演示" class="table-of-contents__link toc-highlight">项目：用Gradio构建一个交互式机器学习演示</a></li><li><a href="#总结" class="table-of-contents__link toc-highlight">总结</a></li><li><a href="#练习" class="table-of-contents__link toc-highlight">练习</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/en/docs/category/learn-javascript-with-p5js">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.facebook.com/taixr" target="_blank" rel="noopener noreferrer" class="footer__link-item">Facebook<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.tiktok.com/@taixr" target="_blank" rel="noopener noreferrer" class="footer__link-item">TikTok<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://space.bilibili.com/1536802703" target="_blank" rel="noopener noreferrer" class="footer__link-item">Bilibili<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/en/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/taixr/portal" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Taixr</div></div></div></footer></div>
</body>
</html>